# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

# Import models
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load dataset
df = pd.read_csv("data.csv")

# Remove unnamed columns (fix CSV formatting issues)
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Display dataset info
print(df.info())

# Check for missing values
print("\nMissing Values Before Handling:")
print(df.isnull().sum())

# Handle missing values
df.dropna(inplace=True)  # Drop rows with missing values

# Verify missing values are handled
print("\nMissing Values After Handling:")
print(df.isnull().sum())

# Encode categorical target variable (Benign=0, Malignant=1)
le = LabelEncoder()
df['diagnosis'] = le.fit_transform(df['diagnosis'])

# Define features (X) and target (y)
X = df.drop(columns=['diagnosis'])  # Exclude the target column
y = df['diagnosis']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)


# Initialize models dictionary with ANN included
models = {
    "Decision Tree": DecisionTreeClassifier(),
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(probability=True),
    "Random Forest": RandomForestClassifier(),
    "Naive Bayes": GaussianNB(),
    "KNN": KNeighborsClassifier(),
    "ANN": Sequential([
        Dense(16, activation='relu', input_shape=(X_train.shape[1],)),
        Dense(8, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
}

# Train models, evaluate performance, and store ROC values
metrics_df = []
roc_curves = {}
best_model_name = None
best_f1_score = 0
best_model = None

# Train and evaluate all models
for name, model in models.items():
    if name == "ANN":
        # For ANN, we compile and train it separately
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=1, validation_data=(X_test, y_test))

        # Evaluate ANN
        _, ann_accuracy = model.evaluate(X_test, y_test)
        y_pred_ann = (model.predict(X_test) > 0.5).astype("int32")
        ann_report = classification_report(y_test, y_pred_ann, output_dict=True)
        ann_precision = ann_report["1"]["precision"]
        ann_recall = ann_report["1"]["recall"]
        ann_f1 = ann_report["1"]["f1-score"]

        # Store ANN metrics
        metrics_df.append(["ANN", ann_accuracy, ann_precision, ann_recall, ann_f1])

        # Compute ROC curve for ANN
        y_probs_ann = model.predict(X_test).ravel()
        fpr_ann, tpr_ann, _ = roc_curve(y_test, y_probs_ann)
        roc_curves["ANN"] = (fpr_ann, tpr_ann, auc(fpr_ann, tpr_ann))
    else:
        # For other models, we fit and evaluate them normally
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Compute metrics
        acc = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)
        precision = report["1"]["precision"]
        recall = report["1"]["recall"]
        f1 = report["1"]["f1-score"]

        print(f"\n{name} Metrics:")
        print(f"  Accuracy: {acc:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall: {recall:.4f}")
        print(f"  F1 Score: {f1:.4f}")

        # Store the best model based on F1-score
        if f1 > best_f1_score:
            best_f1_score = f1
            best_model_name = name
            best_model = model

        # Compute ROC curve
        y_probs = model.predict_proba(X_test)[:, 1]  # Probability scores
        fpr, tpr, _ = roc_curve(y_test, y_probs)
        roc_curves[name] = (fpr, tpr, auc(fpr, tpr))

        # Store metrics
        metrics_df.append([name, acc, precision, recall, f1])

# Compute ROC curve for all models
plt.figure(figsize=(10, 7))
for model_name, (fpr, tpr, roc_auc) in roc_curves.items():
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')

plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for All Models')
plt.legend(loc='lower right')
plt.show()

# Display model comparison metrics
metrics_df = pd.DataFrame(metrics_df, columns=["Model", "Accuracy", "Precision", "Recall", "F1 Score"])
print("\nModel Comparison Metrics:")
print(metrics_df)

# Check the best model based on F1 Score
print(f"\nBest Model: {best_model_name} with F1 Score: {best_f1_score:.4f}")

# Predict on new data (using first test sample)
new_data = np.array([X_test[0]])  # Example prediction
prediction = best_model.predict(new_data)

# Print prediction result
print(f"\nPredicted Diagnosis: {'Malignant' if prediction[0] == 1 else 'Benign'}")
